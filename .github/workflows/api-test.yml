# T√™n c·ªßa quy tr√¨nh CI/CD
name: Laravel CI/CD with Newman Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Which test suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - brands-only
          - favorites-only
          - users-only
      environment:
        description: 'Target environment'
        required: false
        default: 'test'
        type: choice
        options:
          - test
          - staging

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    # B∆∞·ªõc 1: L·∫•y code t·ª´ repository v·ªÅ
    - name: Checkout Code ‚öôÔ∏è
      uses: actions/checkout@v4

    # B∆∞·ªõc 2: T·∫°o file .env cho Laravel
    - name: Create Laravel .env file üîß
      env:
        APP_KEY: ${{ secrets.APP_KEY }}
        DB_DATABASE: ${{ secrets.DB_DATABASE }}
        DB_USERNAME: ${{ secrets.DB_USERNAME }}
        DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
        JWT_SECRET: ${{ secrets.JWT_SECRET }}
      run: |
        echo "üîß Creating Laravel .env file for CI/CD..."
        # Remove any existing .env file
        rm -f sprint5-with-bugs/API/.env
        # Create .env file with proper database configuration
        echo "APP_NAME=Toolshop" >> sprint5-with-bugs/API/.env
        echo "APP_ENV=testing" >> sprint5-with-bugs/API/.env
        echo "APP_KEY=${APP_KEY:-base64:YourTestAppKeyHere123456789012345678901234567890}" >> sprint5-with-bugs/API/.env
        echo "APP_DEBUG=true" >> sprint5-with-bugs/API/.env
        echo "APP_URL=http://localhost:8091" >> sprint5-with-bugs/API/.env
        echo "APP_TIMEZONE=UTC" >> sprint5-with-bugs/API/.env
        echo "" >> sprint5-with-bugs/API/.env
        echo "LOG_CHANNEL=single" >> sprint5-with-bugs/API/.env
        echo "LOG_LEVEL=debug" >> sprint5-with-bugs/API/.env
        echo "" >> sprint5-with-bugs/API/.env
        echo "DB_CONNECTION=mysql" >> sprint5-with-bugs/API/.env
        echo "DB_HOST=mariadb" >> sprint5-with-bugs/API/.env
        echo "DB_PORT=3306" >> sprint5-with-bugs/API/.env
        echo "DB_DATABASE=${DB_DATABASE:-toolshop}" >> sprint5-with-bugs/API/.env
        echo "DB_USERNAME=${DB_USERNAME:-user}" >> sprint5-with-bugs/API/.env
        echo "DB_PASSWORD=${DB_PASSWORD:-root}" >> sprint5-with-bugs/API/.env
        echo "" >> sprint5-with-bugs/API/.env
        echo "CACHE_DRIVER=array" >> sprint5-with-bugs/API/.env
        echo "QUEUE_CONNECTION=sync" >> sprint5-with-bugs/API/.env
        echo "SESSION_DRIVER=array" >> sprint5-with-bugs/API/.env
        echo "" >> sprint5-with-bugs/API/.env
        echo "JWT_SECRET=${JWT_SECRET:-your_test_jwt_secret_key_here_make_it_long_enough}" >> sprint5-with-bugs/API/.env
        echo "" >> sprint5-with-bugs/API/.env
        echo "MAIL_DRIVER=log" >> sprint5-with-bugs/API/.env
        echo "BROADCAST_DRIVER=log" >> sprint5-with-bugs/API/.env
        echo "CACHE_STORE=array" >> sprint5-with-bugs/API/.env
        echo "FILESYSTEM_DRIVER=local" >> sprint5-with-bugs/API/.env
        echo "SESSION_LIFETIME=120" >> sprint5-with-bugs/API/.env
        echo "‚úÖ .env file created successfully."
        echo "üìÑ .env file contents:"
        cat sprint5-with-bugs/API/.env

    # B∆∞·ªõc 3: Kh·ªüi ƒë·ªông c√°c container Docker
    - name: Start Docker Containers üê≥
      env:
        SPRINT_FOLDER: sprint5-with-bugs
        DISABLE_LOGGING: false
      run: docker compose up -d

    # B∆∞·ªõc 4: Ch·ªù c√°c d·ªãch v·ª• s·∫µn s√†ng
    - name: Wait for Services ‚è≥
      run: sleep 60
      shell: bash

    # B∆∞·ªõc 5: C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán v√† thi·∫øt l·∫≠p ·ª©ng d·ª•ng
    - name: Setup Application üîß
      env:
        SPRINT_FOLDER: sprint5-with-bugs
        DISABLE_LOGGING: false
      run: |
        echo "üì¶ Installing Composer dependencies..."
        # S·ª≠ d·ª•ng service 'composer' ƒë√£ ƒë·ªãnh nghƒ©a trong docker-compose.yml
        docker compose run --rm composer
        
        echo "üîí Fixing permissions..."
        # G√°n quy·ªÅn truy c·∫≠p cho th∆∞ m·ª•c storage v√† cache
        docker compose exec -T -u root laravel-api chown -R www-data:www-data /var/www/storage /var/www/bootstrap/cache
        
        echo "üóÑÔ∏è Running database migrations and seeding..."
        # Ch·∫°y migrate & seed
        docker compose exec -T laravel-api php artisan migrate:fresh --seed --force

    # B∆∞·ªõc 6: C√†i ƒë·∫∑t Node.js v√† Newman
    - name: Install Node.js & Newman ‚öôÔ∏è
      uses: actions/setup-node@v4
      with:
        node-version: '18'
    - run: npm install -g newman newman-reporter-htmlextra

    # B∆∞·ªõc 7: Ch·∫°y ki·ªÉm th·ª≠ Newman
    - name: Run Newman Tests üöÄ
      run: |
        echo "üìÅ Creating reports directory..."
        mkdir -p reports
        
        echo "üîç Testing API endpoints with retry..."
        
        # Function to test endpoint with retry
        test_endpoint() {
          local url=$1
          local max_attempts=5
          local attempt=1
          
          while [ $attempt -le $max_attempts ]; do
            echo "  Attempt $attempt/$max_attempts: Testing $url"
            if curl -f "$url" -H "Accept: application/json" --connect-timeout 10 --max-time 30; then
              echo "  ‚úÖ $url is working"
              return 0
            else
              echo "  ‚ùå $url failed (attempt $attempt)"
              if [ $attempt -eq $max_attempts ]; then
                echo "  üö® $url failed after $max_attempts attempts"
                return 1
              fi
              echo "  ‚è≥ Waiting 10 seconds before retry..."
              sleep 10
            fi
            attempt=$((attempt + 1))
          done
        }
        
        # Test endpoints
        test_endpoint "http://localhost:8091/status" || echo "‚ö†Ô∏è Status endpoint not available, continuing anyway..."
        test_endpoint "http://localhost:8091/products" || echo "‚ö†Ô∏è Products endpoint not available, continuing anyway..."
        
        echo "üìã Checking Docker containers status..."
        docker compose ps
        
        echo "üöÄ Running Newman test suites..."
        echo "üìù Selected test suite: ${{ github.event.inputs.test_suite || 'all' }}"
        echo "üåç Target environment: ${{ github.event.inputs.environment || 'test' }}"
        
        # Initialize variables to track test results
        BRANDS_RESULT=0
        FAVORITES_RESULT=0
        USERS_RESULT=0
        
        # Test suite 1: Brands tests
        if [[ "${{ github.event.inputs.test_suite || 'all' }}" == "all" || "${{ github.event.inputs.test_suite || 'all' }}" == "brands-only" ]]; then
          if [ -f "tests/brands-data-driven-collection.json" ] && [ -f "tests/brands-test-data.csv" ]; then
            echo "üìã Running brands tests with CSV data..."
            if newman run tests/brands-data-driven-collection.json \
              --iteration-data tests/brands-test-data.csv \
              --environment tests/environment.json \
              --reporters cli,htmlextra \
              --reporter-htmlextra-export reports/brands-test-report.html \
              --reporter-htmlextra-title "Brands Tests Report" \
              --reporter-htmlextra-showOnlyFails; then
              echo "‚úÖ Brands tests completed successfully"
              BRANDS_RESULT=0
            else
              echo "‚ùå Brands tests failed, but continuing with other tests..."
              BRANDS_RESULT=1
            fi
          else
            echo "‚ö†Ô∏è Brands test files not found, skipping..."
            BRANDS_RESULT=2
          fi
        else
          echo "‚è≠Ô∏è Skipping brands tests (not selected)"
        fi
        
        # Test suite 2: Favorites tests  
        if [[ "${{ github.event.inputs.test_suite || 'all' }}" == "all" || "${{ github.event.inputs.test_suite || 'all' }}" == "favorites-only" ]]; then
          if [ -f "tests/favorites-data-driven-collection.json" ] && [ -f "tests/favorites-test-data.csv" ]; then
            echo "üìã Running favorites tests with CSV data..."
            if newman run tests/favorites-data-driven-collection.json \
              --iteration-data tests/favorites-test-data.csv \
              --environment tests/environment.json \
              --reporters cli,htmlextra \
              --reporter-htmlextra-export reports/favorites-test-report.html \
              --reporter-htmlextra-title "Favorites Tests Report" \
              --reporter-htmlextra-showOnlyFails; then
              echo "‚úÖ Favorites tests completed successfully"
              FAVORITES_RESULT=0
            else
              echo "‚ùå Favorites tests failed, but continuing with other tests..."
              FAVORITES_RESULT=1
            fi
          else
            echo "‚ö†Ô∏è Favorites test files not found, skipping..."
            FAVORITES_RESULT=2
          fi
        else
          echo "‚è≠Ô∏è Skipping favorites tests (not selected)"
        fi
        
        # Test suite 3: Users tests
        if [[ "${{ github.event.inputs.test_suite || 'all' }}" == "all" || "${{ github.event.inputs.test_suite || 'all' }}" == "users-only" ]]; then
          if [ -f "tests/users-data-driven-collection.json" ] && [ -f "tests/users-test-data.csv" ]; then
            echo "üìã Running users tests with CSV data..."
            if newman run tests/users-data-driven-collection.json \
              --iteration-data tests/users-test-data.csv \
              --environment tests/environment.json \
              --reporters cli,htmlextra \
              --reporter-htmlextra-export reports/users-test-report.html \
              --reporter-htmlextra-title "Users Tests Report" \
              --reporter-htmlextra-showOnlyFails; then
              echo "‚úÖ Users tests completed successfully"
              USERS_RESULT=0
            else
              echo "‚ùå Users tests failed, but continuing with other tests..."
              USERS_RESULT=1
            fi
          else
            echo "‚ö†Ô∏è Users test files not found, skipping..."
            USERS_RESULT=2
          fi
        else
          echo "‚è≠Ô∏è Skipping users tests (not selected)"
        fi
        
        # Ch·∫°y th√™m full test collections (n·∫øu c√≥)
        echo "üìã Running additional full test collections..."
        FULL_TESTS_RESULT=0
        for collection in tests/*-full-tests.postman_collection.json; do
          if [ -f "$collection" ]; then
            filename=$(basename "$collection" .postman_collection.json)
            echo "üîÑ Running $filename..."
            if newman run "$collection" \
              --environment tests/environment.json \
              --reporters cli,htmlextra \
              --reporter-htmlextra-export "reports/$filename.html" \
              --reporter-htmlextra-title "$filename Report" \
              --reporter-htmlextra-showOnlyFails; then
              echo "‚úÖ $filename completed successfully"
            else
              echo "‚ùå $filename failed, but continuing..."
              FULL_TESTS_RESULT=1
            fi
          fi
        done
        
        echo "üìä Test Results Summary:"
        echo "=================================="
        echo "Brands Tests: $([ $BRANDS_RESULT -eq 0 ] && echo "‚úÖ PASSED" || [ $BRANDS_RESULT -eq 1 ] && echo "‚ùå FAILED" || echo "‚è≠Ô∏è SKIPPED")"
        echo "Favorites Tests: $([ $FAVORITES_RESULT -eq 0 ] && echo "‚úÖ PASSED" || [ $FAVORITES_RESULT -eq 1 ] && echo "‚ùå FAILED" || echo "‚è≠Ô∏è SKIPPED")"
        echo "Users Tests: $([ $USERS_RESULT -eq 0 ] && echo "‚úÖ PASSED" || [ $USERS_RESULT -eq 1 ] && echo "‚ùå FAILED" || echo "‚è≠Ô∏è SKIPPED")"
        echo "Full Tests: $([ $FULL_TESTS_RESULT -eq 0 ] && echo "‚úÖ PASSED" || echo "‚ùå FAILED")"
        echo "=================================="
        
        echo "üìä Generated reports:"
        ls -la reports/ || echo "No reports generated"
        
        echo "‚úÖ All Newman tests completed!"
        
        # Calculate overall result - only fail if ALL available tests failed
        # Initialize counters for available and failed tests
        AVAILABLE_TESTS=0
        FAILED_TESTS=0
        
        # Count available tests (where files exist and tests were attempted)
        if [ $BRANDS_RESULT -ne 2 ]; then AVAILABLE_TESTS=$((AVAILABLE_TESTS + 1)); fi
        if [ $FAVORITES_RESULT -ne 2 ]; then AVAILABLE_TESTS=$((AVAILABLE_TESTS + 1)); fi
        if [ $USERS_RESULT -ne 2 ]; then AVAILABLE_TESTS=$((AVAILABLE_TESTS + 1)); fi
        if [ $FULL_TESTS_RESULT -ne 0 ]; then AVAILABLE_TESTS=$((AVAILABLE_TESTS + 1)); fi
        
        # Count failed tests (where tests were attempted but failed)
        if [ $BRANDS_RESULT -eq 1 ]; then FAILED_TESTS=$((FAILED_TESTS + 1)); fi
        if [ $FAVORITES_RESULT -eq 1 ]; then FAILED_TESTS=$((FAILED_TESTS + 1)); fi
        if [ $USERS_RESULT -eq 1 ]; then FAILED_TESTS=$((FAILED_TESTS + 1)); fi
        if [ $FULL_TESTS_RESULT -eq 1 ]; then FAILED_TESTS=$((FAILED_TESTS + 1)); fi
        
        echo "üìä Test Summary: $AVAILABLE_TESTS tests available, $FAILED_TESTS tests failed"
        
        # Only fail if ALL available tests failed (and there were tests to run)
        if [ $AVAILABLE_TESTS -gt 0 ] && [ $FAILED_TESTS -eq $AVAILABLE_TESTS ]; then
          echo "üö® Critical: All available tests failed. Failing the workflow."
          exit 1
        else
          echo "‚ÑπÔ∏è Tests completed successfully or some tests were skipped due to missing files."
          exit 0
        fi
        
    # B∆∞·ªõc 8: Upload b√°o c√°o (lu√¥n ch·∫°y)
    - name: Upload Test Reports üìä
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: newman-test-reports
        path: reports/
        retention-days: 30

    # B∆∞·ªõc 9: D·ªçn d·∫πp (lu√¥n ch·∫°y)
    - name: Cleanup üßπ
      if: always()
      env:
        SPRINT_FOLDER: sprint5-with-bugs
        DISABLE_LOGGING: false
      run: docker compose down -v